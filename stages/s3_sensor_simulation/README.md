# Stage 3: Sensor Simulation

## ğŸ“¡ Overview

This stage implements comprehensive sensor simulation for the quadcopter, including RGB camera, lidar, GPS, ultrasonic sensors, and other advanced sensors with realistic noise profiles, latency simulation, and failure modes.

## ğŸ—ï¸ Architecture

### Components
- **RGB Camera**: High-resolution camera with realistic optics and noise
- **Lidar Sensor**: 3D point cloud generation with range and intensity
- **GPS Module**: Position and velocity with realistic accuracy
- **Ultrasonic Sensors**: Short-range obstacle detection
- **IMU Enhancement**: Improved inertial measurement with bias and drift
- **Depth Camera**: RGB-D sensor for 3D perception
- **Thermal Camera**: Infrared imaging for search and rescue

### Key Features
- âœ… Realistic sensor noise and latency profiles
- âœ… Sensor failure simulation and recovery
- âœ… Multi-sensor fusion capabilities
- âœ… Configurable sensor parameters
- âœ… Real-time data processing
- âœ… Sensor calibration and validation

## ğŸš€ Quick Start

### Prerequisites
- Stage 1: Quadcopter Simulation
- Stage 2: 3D Map Creation
- Docker Desktop for Mac

### Installation & Running

```bash
# Navigate to Stage 3
cd stages/s3_sensor_simulation

# Build and run with Docker
docker-compose up --build

# Or run individual components
./scripts/launch_camera_sensor.sh
./scripts/launch_lidar_sensor.sh
./scripts/launch_gps_sensor.sh
```

## ğŸ“ File Structure

```
s3_sensor_simulation/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ camera_sensor/
â”‚   â”œâ”€â”€ lidar_sensor/
â”‚   â”œâ”€â”€ gps_sensor/
â”‚   â”œâ”€â”€ ultrasonic_sensor/
â”‚   â””â”€â”€ sensor_fusion/
â”œâ”€â”€ launch/
â”‚   â”œâ”€â”€ camera.launch.py
â”‚   â”œâ”€â”€ lidar.launch.py
â”‚   â”œâ”€â”€ gps.launch.py
â”‚   â””â”€â”€ all_sensors.launch.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ camera_params.yaml
â”‚   â”œâ”€â”€ lidar_params.yaml
â”‚   â”œâ”€â”€ gps_params.yaml
â”‚   â””â”€â”€ sensor_fusion.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ launch_camera_sensor.sh
â”‚   â”œâ”€â”€ launch_lidar_sensor.sh
â”‚   â””â”€â”€ test_sensors.sh
â””â”€â”€ tests/
    â”œâ”€â”€ test_camera.py
    â”œâ”€â”€ test_lidar.py
    â””â”€â”€ test_sensor_fusion.py
```

## ğŸ“· Sensor Types

### RGB Camera
- **Resolution**: 1920x1080 (configurable)
- **Frame Rate**: 30 FPS
- **Field of View**: 90Â° horizontal
- **Features**: Auto-exposure, noise simulation, lens distortion
- **Topics**: `/quadcopter/sensors/camera/image_raw`, `/quadcopter/sensors/camera/camera_info`

### Lidar Sensor
- **Type**: 3D Velodyne-style lidar
- **Range**: 0.5m - 100m
- **Points**: 64 channels, 360Â° horizontal
- **Features**: Intensity data, range accuracy simulation
- **Topics**: `/quadcopter/sensors/lidar/points`, `/quadcopter/sensors/lidar/scan`

### GPS Module
- **Accuracy**: Â±2m horizontal, Â±5m vertical
- **Update Rate**: 10 Hz
- **Features**: Satellite simulation, multipath effects
- **Topics**: `/quadcopter/sensors/gps/fix`, `/quadcopter/sensors/gps/vel`

### Ultrasonic Sensors
- **Range**: 0.1m - 4m
- **Accuracy**: Â±1cm
- **Update Rate**: 20 Hz
- **Features**: Multi-echo detection, temperature compensation
- **Topics**: `/quadcopter/sensors/ultrasonic/range`

### Depth Camera
- **Resolution**: 640x480
- **Range**: 0.5m - 10m
- **Features**: RGB + depth fusion, noise simulation
- **Topics**: `/quadcopter/sensors/depth/image_raw`, `/quadcopter/sensors/depth/points`

### Thermal Camera
- **Resolution**: 320x240
- **Temperature Range**: -40Â°C to +150Â°C
- **Features**: Heat signature detection, emissivity simulation
- **Topics**: `/quadcopter/sensors/thermal/image_raw`

## ğŸ§ª Testing

### Run All Tests
```bash
./scripts/run_tests.sh
```

### Individual Tests
```bash
# Test camera sensor
python3 tests/test_camera.py

# Test lidar sensor
python3 tests/test_lidar.py

# Test GPS sensor
python3 tests/test_gps.py

# Test sensor fusion
python3 tests/test_sensor_fusion.py
```

## ğŸ”§ Configuration

### Camera Parameters
Edit `config/camera_params.yaml`:
```yaml
camera_sensor:
  resolution:
    width: 1920
    height: 1080
  frame_rate: 30.0
  field_of_view: 90.0
  noise:
    gaussian_std: 0.02
    salt_pepper_prob: 0.001
  distortion:
    k1: 0.1
    k2: 0.05
    p1: 0.001
    p2: 0.001
  exposure:
    auto: true
    gain: 1.0
    exposure_time: 0.033
```

### Lidar Parameters
Edit `config/lidar_params.yaml`:
```yaml
lidar_sensor:
  channels: 64
  horizontal_resolution: 0.1
  vertical_fov: [-15, 15]
  range:
    min: 0.5
    max: 100.0
  accuracy:
    range_std: 0.02
    angle_std: 0.001
  noise:
    gaussian_std: 0.01
    dropout_prob: 0.001
```

### GPS Parameters
Edit `config/gps_params.yaml`:
```yaml
gps_sensor:
  accuracy:
    horizontal_std: 2.0
    vertical_std: 5.0
    velocity_std: 0.1
  update_rate: 10.0
  satellites:
    min_visible: 4
    max_visible: 12
  multipath:
    enabled: true
    probability: 0.1
    delay_std: 0.5
```

## ğŸ”„ Integration with Previous Stages

### Input from Stage 1 & 2
- Quadcopter model and physics
- 3D environment and obstacles
- Flight controller interface
- Basic sensor framework

### Output for Stage 4
- Rich sensor data streams
- Multi-modal perception data
- Sensor fusion capabilities
- Realistic noise and failure modes

## ğŸ“Š Performance Metrics

### Target Performance
- **Camera Processing**: 30 FPS, < 100ms latency
- **Lidar Processing**: 10 Hz, < 50ms latency
- **GPS Processing**: 10 Hz, < 20ms latency
- **Sensor Fusion**: 20 Hz, < 100ms latency
- **Memory Usage**: < 1GB per sensor
- **CPU Usage**: < 30% per sensor

### Sensor Accuracy
- **Camera**: Â±2% intensity, Â±1Â° angle
- **Lidar**: Â±2cm range, Â±0.1Â° angle
- **GPS**: Â±2m horizontal, Â±5m vertical
- **Ultrasonic**: Â±1cm range
- **Depth**: Â±2cm range, Â±1Â° angle

## ğŸ¯ Success Criteria

1. **âœ… Realistic Sensor Data**: Accurate noise and latency profiles
2. **âœ… Multi-Sensor Support**: Camera, lidar, GPS, ultrasonic, depth, thermal
3. **âœ… Sensor Fusion**: Multi-modal data integration
4. **âœ… Failure Simulation**: Realistic sensor failure modes
5. **âœ… Performance**: Real-time processing on M1 MacBook
6. **âœ… Integration**: Works with Stages 1 & 2
7. **âœ… Testing**: Comprehensive sensor validation
8. **âœ… Documentation**: Complete sensor API documentation

## ğŸš€ Next Stage Preparation

This stage provides:
- **Rich sensor data streams** for perception
- **Multi-modal sensor fusion** capabilities
- **Realistic noise and failure** simulation
- **Configurable sensor parameters** for different scenarios

Ready for **Stage 4: SLAM Engine** where we'll implement simultaneous localization and mapping using the sensor data. 